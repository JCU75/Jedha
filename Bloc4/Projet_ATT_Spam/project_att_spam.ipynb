{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygAI10h22a5Q"
   },
   "source": [
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M08-deep-learning/AT%26T_logo_2016.svg\" alt=\"AT&T LOGO\" width=\"50%\" height=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8oo5BOl2rKu"
   },
   "source": [
    "# ATT SPAM DETECTOR\n",
    "## Company's Description ðŸ“‡\n",
    "\n",
    "AT&T Inc. is an American multinational telecommunications holding company headquartered at Whitacre Tower in Downtown Dallas, Texas. It is the world's largest telecommunications company by revenue and the third largest provider of mobile telephone services in the U.S. As of 2022, AT&T was ranked 13th on the Fortune 500 rankings of the largest United States corporations, with revenues of $168.8 billion! ðŸ˜®\n",
    "\n",
    "## Project ðŸš§\n",
    "\n",
    "One of the main pain point that AT&T users are facing is constant exposure to SPAM messages.\n",
    "\n",
    "AT&T has been able to manually flag spam messages for a time, but they are looking for an automated way of detecting spams to protect their users.\n",
    "\n",
    "## Goals ðŸŽ¯\n",
    "\n",
    "Your goal is to build a spam detector, that can automatically flag spams as they come based solely on the sms' content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGcpYWqG3HBn"
   },
   "source": [
    "# 1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jna5HR9v2W4F"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbaKqAVq34Ze"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HN1KkYE38bu",
    "outputId": "cc4a30c6-464d-495e-b76b-87c0c12ac63d"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "TOTUm0C34Hon",
    "outputId": "df8b059f-bfe6-4144-f596-59ec1ea59f02"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "nrWZGFLE4MHM",
    "outputId": "6753ccce-f5d5-4f65-8076-6f8cd3eafed0"
   },
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "2_mkp5_S4QnT",
    "outputId": "ed9c66df-8f82-4ba2-d019-70fb09eb2b56"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "AXLjatRvP3Td",
    "outputId": "bde30f56-bba8-4b42-dcbf-77e9134fc088"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc0qThCN4YLp"
   },
   "source": [
    "\n",
    "\n",
    "*   We can see that data have 3 columns \"Unnamed\" with practically only missing values : we we will delete this columns.\n",
    "*   Let's rename columns v1 and v2 for better understanding: v1 = **label** , v2 = **message**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYQx_GQl5CqW"
   },
   "outputs": [],
   "source": [
    "df = df[['v1', 'v2']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vHeX_ShC5MMb",
    "outputId": "c11bd334-86d2-4f5f-e889-9a01517d6e51"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlEtfXfc5QZR"
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'v1': 'label', 'v2': 'message'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Uy9BWbe_5Vog",
    "outputId": "8eaf128e-e7d5-43ef-a0ef-e8c36420ce38"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcB8C-605aiI"
   },
   "source": [
    "## Now let's check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19_n7a9Y54zN",
    "outputId": "ea1ce95b-f114-4b0b-b123-817c02d5e5a3"
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGnF4LVa6ACr"
   },
   "source": [
    "## Now let's convert values of the column label to numeric\n",
    "\n",
    "*   **ham: 0**\n",
    "*   **spam: 1**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUiIjWC26PBa"
   },
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gIbLN3pk6kQM",
    "outputId": "aad426f0-e9f0-4136-abd1-6f113b877266"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rS5f-1csQb6d"
   },
   "source": [
    "## Now we have to remove punctuation, lower case all characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lQbrBgLQuZb"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RiAYOVe6Q1ZN",
    "outputId": "eb2092d9-fc62-44da-be41-a62c8ced21e6"
   },
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YHLCUZVXlMQ"
   },
   "outputs": [],
   "source": [
    "# Lower case\n",
    "df['lower_message'] = df['message'].fillna('').apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO-0FNuWRG-v"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "df['clean_message'] = df['lower_message'].str.replace(r\"[!\\\"#$%&()*+,-./:;<=>?@[\\\\\\]^_`{|}~]+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DppEXBqSSzUC",
    "outputId": "efee3baa-af8d-42fa-effa-3cef11eb620a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHICW8Tl63Bv"
   },
   "source": [
    "## Let's check if data in columns **label** is balance or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "lZdOcMCE7M2P",
    "outputId": "987b6a22-7b3c-4bfb-8659-c92f59f6b0e3"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prPKvSAi7Rar"
   },
   "source": [
    "\n",
    "\n",
    "*   We can see that we have an unbalances column as expected. It's normal to have more **ham** than **spam** messages\n",
    "*   We will have to use techniques to balance the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQyTjjYr7uyv"
   },
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFBVFLTk8dtn"
   },
   "source": [
    "We will use the weights technique to counterbalance the unbalanced data set to avoid adding artificial data and avoid deleting data. For this we will use **compute_class_weight** provided by **sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70T0VW1m9898",
    "outputId": "a475d8c3-4089-4cdd-fff1-d9784fb71508"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.array([0, 1]) # ham = 0, 1 = spam\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=df['label'])\n",
    "class_weights = dict(zip(classes, weights))\n",
    "print(\"class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIn-29fiAFpP"
   },
   "source": [
    "We have to transform text into number before we can use it for the model. For this we will use **TfidfVectorizer** provided by **sklearn** for **Natural Language Processing**(NLP). TfidfVectorizer is a feature extraction technique for converting a collection of raw text documents into a **matrix** of **TF-IDF** (Term Frequency-Inverse Document Frequency) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40qLDH7VCBdM",
    "outputId": "ddba1535-f028-4ead-8450-9641a3e46fe5"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X = vectorizer.fit_transform(df['clean_message'])\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wk7Q9Lg8RJH"
   },
   "source": [
    "# 3. Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fZzFWljCxNq",
    "outputId": "d19854de-7a08-4aa4-85b6-0ef680199419"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSTmNC18D5c1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "MUNNAm5cD_wr",
    "outputId": "b7a85847-9d3a-480a-c3ca-3ced0e2f51a9"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XauzignCEGnN"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "OV9salosElB3",
    "outputId": "2575bff3-d5bc-402d-a1b4-a4956672a2fa"
   },
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWlaNcM3Eppd"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zNpLJwYEuAp",
    "outputId": "a804d9f9-06ef-4ca3-a41e-390394f3a228"
   },
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6-aXZHsE6Ej",
    "outputId": "788784e1-9cae-4646-b6da-28621500890a"
   },
   "outputs": [],
   "source": [
    "# Model training\n",
    "history = model.fit(\n",
    "    X_train.toarray(), y_train,  # We convert X_train to an array because TensorFlow does not support sparse matrices\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test.toarray(), y_test),\n",
    "    class_weight=class_weights  # use of class_weights calculate before\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIea3tZiiVa1"
   },
   "source": [
    "Let's save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cqaq_KcsqPWD"
   },
   "outputs": [],
   "source": [
    "model.save('/spam_baseline.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILGQ6XH4F1D0"
   },
   "source": [
    "Now let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIdp-ScqF4xC",
    "outputId": "548aaaa3-24b5-43af-fd24-9fabcf562e5d"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test.toarray(), y_test)\n",
    "print(f\"Test accuracy: {accuracy:.4f} - Test Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22PFQsTVGoF0",
    "outputId": "57a2b6b5-f96a-4d47-a900-59ecebd20c82"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = (model.predict(X_test.toarray()) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P687UIpeHluB",
    "outputId": "1b03fa94-497b-42ce-b09a-196c0e10b1d1"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test.toarray())\n",
    "y_pred = (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "6_-tW_ZjIpd5",
    "outputId": "24246858-272c-419a-863f-3bb7fba895c0"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "cm1 = confusion_matrix(y_test, y_pred)\n",
    "fig = px.imshow(\n",
    "    cm1,\n",
    "    labels=dict(x=\"Predict\", y=\"Reel\", color=\"Count\"),\n",
    "    x=[\"Ham\", \"Spam\"],\n",
    "    y=[\"Ham\", \"Spam\"],\n",
    "    text_auto=True,\n",
    "    color_continuous_scale='Blackbody'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Confusion Matrix\",\n",
    "    xaxis_title=\"Prediction\",\n",
    "    yaxis_title=\"Reel\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyMIeLY53eQA"
   },
   "source": [
    "# 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h28XhMK_JH5H"
   },
   "source": [
    "For transfer learning, we will use a model host in Huggingface. The model that we will use is a model made by Mr Michael Shenoda for text classification. This model use RoBerta (Robustly Optimized BERT Pretraining Approach) built on BERT and modifies key hyperparameters, removing the next sentence pretraining objective and trainning with much larger mini batches and learning rates. (https://huggingface.co/docs/transformers/model_doc/roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455,
     "referenced_widgets": [
      "254749774adf479d855ee5bb255abd53",
      "975fcc5aa1a24bc197d24f5247995087",
      "5e7defbb491d4c8e9d99a150437a4cbf",
      "49d8eeeb28be46d9b780dd6b90d52733",
      "45c2a85c3eee4458a1cb37c2e658e002",
      "e2176265143c4843873c9d87a0205bc5",
      "fa205cc3338c450c9b6c74d6ff6f201b",
      "5923e8c387e64966a2ec33d168ff86a7",
      "15dd5fcb3612449988e6043d7c5d037b",
      "34fde1c13d0546d1a46fa78c1806764f",
      "0a1f43899b4d4081850c2d7edf11518c",
      "b242e5a1d1bf4c72a8df109192be44c6",
      "853a7a2ed43d4fb89d2c55994a8a883b",
      "de74f23397cf49538b816dc50fbc02bf",
      "5ed1789f2e144b17ae4ad2ce6757ff92",
      "56d7a6553f8643adb86059c51ec0726a",
      "9690b13acdfd4ffb9bc48677e27a30aa",
      "369e616fd42240cf9a8276e1409690d1",
      "dd3fce952b16465a8bad39bfd116d99f",
      "1ccc435d57dd485eb366e538db701ff2",
      "0efe6e2f468a44d185c8f4da9772ad87",
      "e90a147030194f8dadf354da7a0308c8",
      "fb39970ad9e54162a2775bac0947b0ec",
      "db3b34f332364298a9031cae51364b59",
      "dd6f652a11504258b3aa342593baa086",
      "2b21ae3256854edea583938814323315",
      "12e07548fce84d01ab4c3bbe68dd73d8",
      "abb3c6f727594ef382820376e709cc35",
      "edab7873c66d420e9cc1e07f98aa5ccc",
      "2dfd88c090ed4338a4aa913e6322b5f5",
      "f595ef5961044532882a97237db99967",
      "da986aa035af492181ea5f9e0ddbdfb3",
      "a2ef137c1a104fc29b49176fa55f42a1",
      "3c1af0668b5f43de91a032edb0aed1b0",
      "3d13f37e81b4415097dc915b19c3aafb",
      "8c8536f81e964d7188544458e9e9d8b7",
      "ccd30682d8af4e5f86066c51104f7040",
      "183821260ba146cda24aa8f0a8b2fdc1",
      "4527ceb89f4a41e2b9e86d230ad75c02",
      "632278a4d3814c909d77d15b801185ca",
      "ba00962018a445e388552f9a79d4c418",
      "dee240cdf2ef4fbcb0afd075660bcabb",
      "ca29a80f9e86417f97504eff89026fdc",
      "827f6d1d672444269093e95561303c3e",
      "bca7753a0e66429daecdf70ae0d75f3f",
      "1442848284674564b79d360b2df6ccc6",
      "2b20952b434643239157315302cb6a9d",
      "6bc0ae25b54d4be183c36d563e55d173",
      "0cadbd1665884481b6a02052c95d4108",
      "cfc0e6dc450a47b9838fbae3a59dfe0e",
      "9f00a5a591474fc09a3c37df344a49e2",
      "f3085a13e8164893adfd78c70fa2046a",
      "2edce3494f434bfda0a9b55ab222c4d7",
      "2b2f982fc35640eb8e435e1ecdf00045",
      "fd48f7ca3f344286a2f7676c95969f55",
      "f56b6c3c907c4452bacab51f21a87051",
      "2d1e0a271fbd409da6b527c8da81bb52",
      "9536d1964cbe43f3a477fc676fac23e5",
      "021e1de143494cf58ad1c64d6cb3a24d",
      "8189e58fcc604c17ab1fdfde90741ae7",
      "c6959b9a688b4205add6c746bd9096bc",
      "edf063259c9446609d8e3222f8cedf18",
      "d2bfd973420b4b1386dc5d3859485a96",
      "f07aa6faa50349cc9b943ee7356ce2b9",
      "c17b4360683944f288a1b1cc7f94eb77",
      "3d872129df6b4ab2a645d6eeb6a1510b"
     ]
    },
    "id": "7qPAJC95APMl",
    "outputId": "806ecd10-47f3-4b10-f9da-065e88fc9455"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load model and tokenizer from huggingface\n",
    "model_name = \"mshenoda/roberta-spam\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Check for GPU use\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7moOYb0aGI39"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\",  encoding='latin-1')\n",
    "df = df[['v1', 'v2']].copy()\n",
    "df.columns = ['label', 'message']\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FmuOMEDwGnXE",
    "outputId": "da9e6c59-2462-42c6-bb8e-39275f0a1a6f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(\n",
    "    list(X_test), padding=True, truncation=True, max_length=512, return_tensors=\"tf\"\n",
    ")\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(encodings))).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4UCzV2vGsGa",
    "outputId": "92546709-b8fb-433f-cf84-192b578d1753"
   },
   "outputs": [],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIEyVJmXGvth",
    "outputId": "83cf3291-5467-48a4-831d-c0ddbba4dd22"
   },
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cP2P8wrzG787",
    "outputId": "865d7ce8-3725-49a2-9a23-cc601dca98e6"
   },
   "outputs": [],
   "source": [
    "# model predictions\n",
    "y_pred_logits = model.predict(test_dataset).logits\n",
    "\n",
    "# Convert logits into classes (0 or 1)\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "y_true = y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZN9AnqHx_5l2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzlBpxx81bVu",
    "outputId": "1f747415-57d9-4e18-9b8e-a4be47841077"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we get good scores, we don't need to do any fine tuning here, just inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "7j2mjQR1MIsH",
    "outputId": "75b73535-b05e-42de-c065-4f485322f621"
   },
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_true, y_pred)\n",
    "fig = px.imshow(\n",
    "    cm2,\n",
    "    labels=dict(x=\"Predict\", y=\"Reel\", color=\"Count\"),\n",
    "    x=[\"Ham\", \"Spam\"],\n",
    "    y=[\"Ham\", \"Spam\"],\n",
    "    text_auto=True,\n",
    "    color_continuous_scale='Blackbody'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Confusion Matrix\",\n",
    "    xaxis_title=\"Prediction\",\n",
    "    yaxis_title=\"Reel\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blZVZe-tFEau"
   },
   "source": [
    "Save the model & tokenizer : to save the model we will use **save_pretrained** provided from transformers library and is the recommanded way to save models from this library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS1xAnGbFPM_",
    "outputId": "59d377a7-a835-4ebc-816a-c294dab5d44a"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"spam_tl\")\n",
    "tokenizer.save_pretrained(\"spam_tokenizer_tl\")\n",
    "print(\"Model & tokenizer saved !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "87kEUfgDZCAT",
    "outputId": "5bebce1a-eece-4de9-c995-19d2707e70b4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Confusion matrix baseline model\n",
    "sns.heatmap(cm1, annot=True, fmt='d', cmap=\"gnuplot\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"], ax=axes[0])\n",
    "axes[0].set_title(\"Confusion matrix - Baseline\")\n",
    "axes[0].set_xlabel(\"Predict\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "\n",
    "# Confusion matrix transfer learning\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap=\"gnuplot\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"], ax=axes[1])\n",
    "axes[1].set_title(\"Confusion matrix - Transfer Learning\")\n",
    "axes[1].set_xlabel(\"Predict\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "# Show charts\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bghb49KEI0R"
   },
   "source": [
    "# 5. Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QThMUO87EWSg",
    "outputId": "c3896270-cf6c-4666-931d-8635eaadca49"
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzPG-w_HEs5V"
   },
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwZl7Q3dpj1V"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAwcctbJGij-"
   },
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0YopuDcGRyI",
    "outputId": "9bfac9ce-c2eb-4e0b-ecc2-c00f0d2ceb7a"
   },
   "outputs": [],
   "source": [
    "from transformers import TFRobertaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(\"spam_tl\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"spam_tokenizer_tl\")\n",
    "print(\"Model & tokenizer loaded !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWnKv6JZG6x2"
   },
   "outputs": [],
   "source": [
    "def predict_spam(text):\n",
    "    # Text Tokenization\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Prediction with the model\n",
    "    logits = model(inputs.data)[0]\n",
    "\n",
    "    # Get predicted class (0 = Ham, 1 = Spam)\n",
    "    pred_class = np.argmax(logits, axis=1)[0]\n",
    "\n",
    "    result=\"\"\n",
    "    if pred_class == 1:\n",
    "        result = \"Spam ðŸ›‘\"\n",
    "    else:\n",
    "        result = \"Ham âœ…\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "Ta6hnYw7G-zi",
    "outputId": "c8f7f75d-ccae-4512-9bd4-85c2246a7e66"
   },
   "outputs": [],
   "source": [
    "app = gr.Interface(\n",
    "    fn=predict_spam,\n",
    "    inputs=gr.Textbox(label=\"Enter a message \"),\n",
    "    outputs=gr.Label(label=\"Result\"),\n",
    "    title=\"Spam Detector with Transfer Learning\",\n",
    "    description=\"Enter a message to see if it is detected as 'Spam' or 'Ham'!\"\n",
    ")\n",
    "\n",
    "app.launch(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
